<<<<<<< HEAD
ComparaÃ§Ã£o de Nove Algoritmos ClÃ¡ssicos de Aprendizado de MÃ¡quina em Conjuntos de Dados de ReferÃªncia
Este repositÃ³rio contÃ©m os resultados de um estudo comparativo aprofundado sobre o desempenho de nove algoritmos clÃ¡ssicos de aprendizado de mÃ¡quina em diversos conjuntos de dados de referÃªncia. O objetivo foi investigar como as caracterÃ­sticas intrÃ­nsecas dos dados influenciam a performance dos modelos.

ğŸ“„ Sobre o Estudo
A seleÃ§Ã£o de algoritmos de aprendizado de mÃ¡quina Ã© crucial em problemas reais, pois o desempenho pode variar significativamente. Este estudo compara nove algoritmos clÃ¡ssicos (Ãrvores de DecisÃ£o, RegressÃ£o Linear, RegressÃ£o LogÃ­stica, Redes Neurais Artificiais (MLP), Perceptron, SVM, Classificadores Bayesianos (GaussianNB), KNN e Random Forest) em nove conjuntos de dados pÃºblicos amplamente utilizados na literatura (Iris, Wine, Breast Cancer Wisconsin, Digits, Diabetes, Heart Disease, Parkinson, Titanic e Bank Marketing).

A avaliaÃ§Ã£o foi realizada utilizando a mÃ©trica de acurÃ¡cia mÃ©dia e testes estatÃ­sticos como o de Friedman e Nemenyi para garantir a robustez das conclusÃµes.

ğŸš€ Metodologia Experimental
Todos os experimentos foram conduzidos utilizando a biblioteca scikit-learn (versÃ£o 1.4.2).

Algoritmos Avaliados
Os seguintes algoritmos foram empregados:

Ãrvores de DecisÃ£o (DecisionTreeClassifier)
RegressÃ£o Linear (LinearRegression)
RegressÃ£o LogÃ­stica (LogisticRegression)
Redes Neurais Artificiais (MLPClassifier)
Perceptron (Perceptron)
SVM (SVC)
Classificadores Bayesianos (GaussianNB)
KNN (KNeighborsClassifier)
Random Forest (RandomForestClassifier)
HiperparÃ¢metros
Para a maioria dos algoritmos, foram utilizados os valores padrÃ£o definidos pela scikit-learn. Ajustes mÃ­nimos foram feitos para garantir a convergÃªncia e estabilidade em alguns casos (ex: max_iter para LogisticRegression, MLPClassifier e Perceptron). NÃ£o foi realizado um ajuste fino exaustivo de hiperparÃ¢metros.

Conjuntos de Dados
Os dados foram obtidos do repositÃ³rio UCI e da prÃ³pria scikit-learn.

Procedimentos
Para garantir uma avaliaÃ§Ã£o robusta, foram adotados os seguintes procedimentos:

ValidaÃ§Ã£o cruzada estratificada com 10 folds: Assegura a manutenÃ§Ã£o da proporÃ§Ã£o das classes.
NormalizaÃ§Ã£o de atributos: Aplicada sempre que cabÃ­vel para evitar distorÃ§Ãµes.
MÃ©trica de desempenho: AcurÃ¡cia mÃ©dia.
AnÃ¡lise estatÃ­stica: Teste de Friedman e teste post-hoc de Nemenyi para identificar diferenÃ§as significativas.
Ferramentas
As bibliotecas utilizadas foram:

scikit-learn
pandas
seaborn
matplotlib
ğŸ“Š Resultados e DiscussÃ£o
Os resultados demonstram que conjuntos de dados com menor complexidade (como Iris e Wine) apresentam desempenho superior e mais consistente entre os algoritmos. Conjuntos mais complexos (como Digits, Titanic e Bank Marketing) exibem maior variabilidade de desempenho, ressaltando a dependÃªncia da escolha do algoritmo ideal em relaÃ§Ã£o Ã  complexidade dos dados, recursos computacionais e requisitos de precisÃ£o da aplicaÃ§Ã£o.

Uma tabela detalhada comparando o desempenho de cada algoritmo em cada dataset, incluindo mÃ©tricas como AcurÃ¡cia, Precision (Weighted Avg), Recall (Weighted Avg), F1-Score (Weighted Avg) e MSE (para regressÃ£o), pode ser encontrada nos resultados do estudo.

ğŸ‘¨â€ğŸ”¬ Autor
Saulo Pereira da Silva

FACOM â€“ Faculdade de ComputaÃ§Ã£o - Universidade Federal de Mato Grosso do Sul (UFMS) CEP 79070-900 â€“ Campo Grande â€“ MS - Brasil

Email: pereira.saulo@ufms.br

ğŸ¤ Como Contribuir
Sinta-se Ã  vontade para explorar o cÃ³digo-fonte, reproduzir os experimentos e contribuir com insights ou melhorias. Se vocÃª tiver alguma sugestÃ£o ou encontrar algum problema, abra uma issue ou envie um pull request.
=======
# comparacao_algoritmos
>>>>>>> 39f834e5f70a3f3a02de62ddc8398c709f4995da
