Compara√ß√£o de Nove Algoritmos Cl√°ssicos de Aprendizado de M√°quina em Conjuntos de Dados de Refer√™ncia
Este reposit√≥rio cont√©m os resultados de um estudo comparativo aprofundado sobre o desempenho de nove algoritmos cl√°ssicos de aprendizado de m√°quina em diversos conjuntos de dados de refer√™ncia. O objetivo foi investigar como as caracter√≠sticas intr√≠nsecas dos dados influenciam a performance dos modelos.

üìÑ Sobre o Estudo
A sele√ß√£o de algoritmos de aprendizado de m√°quina √© crucial em problemas reais, pois o desempenho pode variar significativamente. Este estudo compara nove algoritmos cl√°ssicos (√Årvores de Decis√£o, Regress√£o Linear, Regress√£o Log√≠stica, Redes Neurais Artificiais (MLP), Perceptron, SVM, Classificadores Bayesianos (GaussianNB), KNN e Random Forest) em nove conjuntos de dados p√∫blicos amplamente utilizados na literatura (Iris, Wine, Breast Cancer Wisconsin, Digits, Diabetes, Heart Disease, Parkinson, Titanic e Bank Marketing).

A avalia√ß√£o foi realizada utilizando a m√©trica de acur√°cia m√©dia e testes estat√≠sticos como o de Friedman e Nemenyi para garantir a robustez das conclus√µes.

üöÄ Metodologia Experimental
Todos os experimentos foram conduzidos utilizando a biblioteca scikit-learn (vers√£o 1.4.2).

Algoritmos Avaliados
Os seguintes algoritmos foram empregados:

√Årvores de Decis√£o (DecisionTreeClassifier)
Regress√£o Linear (LinearRegression)
Regress√£o Log√≠stica (LogisticRegression)
Redes Neurais Artificiais (MLPClassifier)
Perceptron (Perceptron)
SVM (SVC)
Classificadores Bayesianos (GaussianNB)
KNN (KNeighborsClassifier)
Random Forest (RandomForestClassifier)
Hiperpar√¢metros
Para a maioria dos algoritmos, foram utilizados os valores padr√£o definidos pela scikit-learn. Ajustes m√≠nimos foram feitos para garantir a converg√™ncia e estabilidade em alguns casos (ex: max_iter para LogisticRegression, MLPClassifier e Perceptron). N√£o foi realizado um ajuste fino exaustivo de hiperpar√¢metros.

Conjuntos de Dados
Os dados foram obtidos do reposit√≥rio UCI e da pr√≥pria scikit-learn.

Procedimentos
Para garantir uma avalia√ß√£o robusta, foram adotados os seguintes procedimentos:

Valida√ß√£o cruzada estratificada com 10 folds: Assegura a manuten√ß√£o da propor√ß√£o das classes.
Normaliza√ß√£o de atributos: Aplicada sempre que cab√≠vel para evitar distor√ß√µes.
M√©trica de desempenho: Acur√°cia m√©dia.
An√°lise estat√≠stica: Teste de Friedman e teste post-hoc de Nemenyi para identificar diferen√ßas significativas.
Ferramentas
As bibliotecas utilizadas foram:

scikit-learn
pandas
seaborn
matplotlib
üìä Resultados e Discuss√£o
Os resultados demonstram que conjuntos de dados com menor complexidade (como Iris e Wine) apresentam desempenho superior e mais consistente entre os algoritmos. Conjuntos mais complexos (como Digits, Titanic e Bank Marketing) exibem maior variabilidade de desempenho, ressaltando a depend√™ncia da escolha do algoritmo ideal em rela√ß√£o √† complexidade dos dados, recursos computacionais e requisitos de precis√£o da aplica√ß√£o.

Uma tabela detalhada comparando o desempenho de cada algoritmo em cada dataset, incluindo m√©tricas como Acur√°cia, Precision (Weighted Avg), Recall (Weighted Avg), F1-Score (Weighted Avg) e MSE (para regress√£o), pode ser encontrada nos resultados do estudo.

üë®‚Äçüî¨ Autor
Saulo Pereira da Silva

FACOM ‚Äì Faculdade de Computa√ß√£o - Universidade Federal de Mato Grosso do Sul (UFMS) CEP 79070-900 ‚Äì Campo Grande ‚Äì MS - Brasil

Email: pereira.saulo@ufms.br

ü§ù Como Contribuir
Sinta-se √† vontade para explorar o c√≥digo-fonte, reproduzir os experimentos e contribuir com insights ou melhorias. Se voc√™ tiver alguma sugest√£o ou encontrar algum problema, abra uma issue ou envie um pull request.